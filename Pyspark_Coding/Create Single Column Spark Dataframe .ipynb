{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pyspark' from '/Users/shubhangigupta/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/__init__.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/23 10:48:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('Word Count').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages_list = [21,23,24,25,26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createDataFrame in module pyspark.sql.session:\n",
      "\n",
      "createDataFrame(data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame method of pyspark.sql.session.SparkSession instance\n",
      "    Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
      "    \n",
      "    When ``schema`` is a list of column names, the type of each column\n",
      "    will be inferred from ``data``.\n",
      "    \n",
      "    When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      "    from ``data``, which should be an RDD of either :class:`Row`,\n",
      "    :class:`namedtuple`, or :class:`dict`.\n",
      "    \n",
      "    When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n",
      "    the real data, or an exception will be thrown at runtime. If the given schema is not\n",
      "    :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "    :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\".\n",
      "    Each record will also be wrapped into a tuple, which can be converted to row later.\n",
      "    \n",
      "    If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
      "    rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
      "    \n",
      "    .. versionadded:: 2.0.0\n",
      "    \n",
      "    .. versionchanged:: 2.1.0\n",
      "       Added verifySchema.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data : :class:`RDD` or iterable\n",
      "        an RDD of any kind of SQL data representation (:class:`Row`,\n",
      "        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n",
      "        :class:`pandas.DataFrame`.\n",
      "    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      "        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "        column names, default is None.  The data type string format equals to\n",
      "        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "        omit the ``struct<>``.\n",
      "    samplingRatio : float, optional\n",
      "        the sample ratio of rows used for inferring\n",
      "    verifySchema : bool, optional\n",
      "        verify data types of every row against schema. Enabled by default.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> l = [('Alice', 1)]\n",
      "    >>> spark.createDataFrame(l).collect()\n",
      "    [Row(_1='Alice', _2=1)]\n",
      "    >>> spark.createDataFrame(l, ['name', 'age']).collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> d = [{'name': 'Alice', 'age': 1}]\n",
      "    >>> spark.createDataFrame(d).collect()\n",
      "    [Row(age=1, name='Alice')]\n",
      "    \n",
      "    >>> rdd = sc.parallelize(l)\n",
      "    >>> spark.createDataFrame(rdd).collect()\n",
      "    [Row(_1='Alice', _2=1)]\n",
      "    >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n",
      "    >>> df.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> Person = Row('name', 'age')\n",
      "    >>> person = rdd.map(lambda r: Person(*r))\n",
      "    >>> df2 = spark.createDataFrame(person)\n",
      "    >>> df2.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> from pyspark.sql.types import *\n",
      "    >>> schema = StructType([\n",
      "    ...    StructField(\"name\", StringType(), True),\n",
      "    ...    StructField(\"age\", IntegerType(), True)])\n",
      "    >>> df3 = spark.createDataFrame(rdd, schema)\n",
      "    >>> df3.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      "    [Row(name='Alice', age=1)]\n",
      "    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      "    [Row(0=1, 1=2)]\n",
      "    \n",
      "    >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
      "    [Row(a='Alice', b=1)]\n",
      "    >>> rdd = rdd.map(lambda row: row[1])\n",
      "    >>> spark.createDataFrame(rdd, \"int\").collect()\n",
      "    [Row(value=1)]\n",
      "    >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "    Traceback (most recent call last):\n",
      "        ...\n",
      "    Py4JJavaError: ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##We can't create DataFrame with list of integers or Strings without infer schema or DataTypes or converting it into the tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: int]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    " #spark.createDataFrame(ages_list)\n",
    "spark.createDataFrame(ages_list,'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: int]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "spark.createDataFrame(ages_list, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"ram\",\"joe\",\"zack\",\"happy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|  ram|\n",
      "|  joe|\n",
      "| zack|\n",
      "|happy|\n",
      "+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_string = spark.createDataFrame(names,'string')\n",
    "df_string.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|  ram|\n",
      "|  joe|\n",
      "| zack|\n",
      "|happy|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "df2 = spark.createDataFrame(names,StringType())\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##WE CAN USE TUPLES TO CREATE SINGLE COLUMN DATAFRAME WITHOUT SPECIFYING THE SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages_list=[(21,),(22,),(23,),(24,)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| _1|\n",
      "+---+\n",
      "| 21|\n",
      "| 22|\n",
      "| 23|\n",
      "| 24|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(ages_list)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##But if we create tuple then specifying schema will be an issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "field value: IntegerType() can not accept object (21,) in type <class 'tuple'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/shubhangigupta/Desktop/Pyspark_Coding/Create Single Column Spark Dataframe Using List.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/shubhangigupta/Desktop/Pyspark_Coding/Create%20Single%20Column%20Spark%20Dataframe%20Using%20List.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m spark\u001b[39m.\u001b[39;49mcreateDataFrame(ages_list,\u001b[39m'\u001b[39;49m\u001b[39mint\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/sql/session.py:894\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[39mif\u001b[39;00m has_pandas \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pandas\u001b[39m.\u001b[39mDataFrame):\n\u001b[1;32m    890\u001b[0m     \u001b[39m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m    891\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(SparkSession, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mcreateDataFrame(  \u001b[39m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m    893\u001b[0m     )\n\u001b[0;32m--> 894\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dataframe(\n\u001b[1;32m    895\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    896\u001b[0m )\n",
      "File \u001b[0;32m~/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/sql/session.py:936\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    934\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_createFromRDD(data\u001b[39m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m    935\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_createFromLocal(\u001b[39mmap\u001b[39;49m(prepare, data), schema)\n\u001b[1;32m    937\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    938\u001b[0m jrdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mSerDeUtil\u001b[39m.\u001b[39mtoJavaArray(rdd\u001b[39m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m~/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/sql/session.py:628\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[39m# make sure data could consumed multiple times\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mlist\u001b[39m):\n\u001b[0;32m--> 628\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(data)\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    631\u001b[0m     struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferSchemaFromList(data, names\u001b[39m=\u001b[39mschema)\n",
      "File \u001b[0;32m~/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/sql/session.py:925\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe.<locals>.prepare\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[39m@no_type_check\u001b[39m\n\u001b[1;32m    924\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare\u001b[39m(obj):\n\u001b[0;32m--> 925\u001b[0m     verify_func(obj)\n\u001b[1;32m    926\u001b[0m     \u001b[39mreturn\u001b[39;00m (obj,)\n",
      "File \u001b[0;32m~/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/sql/types.py:1722\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1720\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mverify\u001b[39m(obj: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1721\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 1722\u001b[0m         verify_value(obj)\n",
      "File \u001b[0;32m~/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/sql/types.py:1635\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_integer\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1633\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mverify_integer\u001b[39m(obj: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1634\u001b[0m     assert_acceptable_types(obj)\n\u001b[0;32m-> 1635\u001b[0m     verify_acceptable_types(obj)\n\u001b[1;32m   1636\u001b[0m     \u001b[39mif\u001b[39;00m obj \u001b[39m<\u001b[39m \u001b[39m-\u001b[39m\u001b[39m2147483648\u001b[39m \u001b[39mor\u001b[39;00m obj \u001b[39m>\u001b[39m \u001b[39m2147483647\u001b[39m:\n\u001b[1;32m   1637\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(new_msg(\u001b[39m\"\u001b[39m\u001b[39mobject of IntegerType out of range, got: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m obj))\n",
      "File \u001b[0;32m~/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/sql/types.py:1592\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_acceptable_types\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1589\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mverify_acceptable_types\u001b[39m(obj: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1590\u001b[0m     \u001b[39m# subclass of them can not be fromInternal in JVM\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(obj) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _acceptable_types[_type]:\n\u001b[0;32m-> 1592\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   1593\u001b[0m             new_msg(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m can not accept object \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m in type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (dataType, obj, \u001b[39mtype\u001b[39m(obj)))\n\u001b[1;32m   1594\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: field value: IntegerType() can not accept object (21,) in type <class 'tuple'>"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(ages_list,'int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create the dataframe with tuple and datatype if we also provide column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: int]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(ages_list,' age int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list = [(1,'Scott'),(2,'Donald'), (3,'Mickey'), (4,'Bunny')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| _1|    _2|\n",
      "+---+------+\n",
      "|  1| Scott|\n",
      "|  2|Donald|\n",
      "|  3|Mickey|\n",
      "|  4| Bunny|\n",
      "+---+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(user_list)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|user_id|username|\n",
      "+-------+--------+\n",
      "|      1|   Scott|\n",
      "|      2|  Donald|\n",
      "|      3|  Mickey|\n",
      "|      4|   Bunny|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(user_list,'user_id int, username string')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark DataFrame is collection of row objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect will show a Dataframe into python list\n",
    "df.collect()\n",
    "type(df.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Row in module pyspark.sql.types:\n",
      "\n",
      "class Row(builtins.tuple)\n",
      " |  Row(*args: Optional[str], **kwargs: Optional[Any]) -> 'Row'\n",
      " |  \n",
      " |  A row in :class:`DataFrame`.\n",
      " |  The fields in it can be accessed:\n",
      " |  \n",
      " |  * like attributes (``row.key``)\n",
      " |  * like dictionary values (``row[key]``)\n",
      " |  \n",
      " |  ``key in row`` will search through row keys.\n",
      " |  \n",
      " |  Row can be used to create a row object by using named arguments.\n",
      " |  It is not allowed to omit a named argument to represent that the value is\n",
      " |  None or missing. This should be explicitly set to None in this case.\n",
      " |  \n",
      " |  .. versionchanged:: 3.0.0\n",
      " |      Rows created from named arguments no longer have\n",
      " |      field names sorted alphabetically and will be ordered in the position as\n",
      " |      entered.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> row = Row(name=\"Alice\", age=11)\n",
      " |  >>> row\n",
      " |  Row(name='Alice', age=11)\n",
      " |  >>> row['name'], row['age']\n",
      " |  ('Alice', 11)\n",
      " |  >>> row.name, row.age\n",
      " |  ('Alice', 11)\n",
      " |  >>> 'name' in row\n",
      " |  True\n",
      " |  >>> 'wrong_key' in row\n",
      " |  False\n",
      " |  \n",
      " |  Row also can be used to create another Row like class, then it\n",
      " |  could be used to create Row objects, such as\n",
      " |  \n",
      " |  >>> Person = Row(\"name\", \"age\")\n",
      " |  >>> Person\n",
      " |  <Row('name', 'age')>\n",
      " |  >>> 'name' in Person\n",
      " |  True\n",
      " |  >>> 'wrong_key' in Person\n",
      " |  False\n",
      " |  >>> Person(\"Alice\", 11)\n",
      " |  Row(name='Alice', age=11)\n",
      " |  \n",
      " |  This form can also be used to create rows as tuple values, i.e. with unnamed\n",
      " |  fields.\n",
      " |  \n",
      " |  >>> row1 = Row(\"Alice\", 11)\n",
      " |  >>> row2 = Row(name=\"Alice\", age=11)\n",
      " |  >>> row1 == row2\n",
      " |  True\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Row\n",
      " |      builtins.tuple\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, *args: Any) -> 'Row'\n",
      " |      create new Row object\n",
      " |  \n",
      " |  __contains__(self, item: Any) -> bool\n",
      " |      Return key in self.\n",
      " |  \n",
      " |  __getattr__(self, item: str) -> Any\n",
      " |  \n",
      " |  __getitem__(self, item: Any) -> Any\n",
      " |      Return self[key].\n",
      " |  \n",
      " |  __reduce__(self) -> Union[str, Tuple[Any, ...]]\n",
      " |      Returns a tuple so Python knows how to pickle Row.\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Printable representation of Row used in Python REPL.\n",
      " |  \n",
      " |  __setattr__(self, key: Any, value: Any) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  asDict(self, recursive: bool = False) -> Dict[str, Any]\n",
      " |      Return as a dict\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      recursive : bool, optional\n",
      " |          turns the nested Rows to dict (default: False).\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If a row contains duplicate field names, e.g., the rows of a join\n",
      " |      between two :class:`DataFrame` that both have the fields of same names,\n",
      " |      one of the duplicate fields will be selected by ``asDict``. ``__getitem__``\n",
      " |      will also return one of the duplicate fields, however returned value might\n",
      " |      be different to ``asDict``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> Row(name=\"Alice\", age=11).asDict() == {'name': 'Alice', 'age': 11}\n",
      " |      True\n",
      " |      >>> row = Row(key=1, value=Row(name='a', age=2))\n",
      " |      >>> row.asDict() == {'key': 1, 'value': Row(name='a', age=2)}\n",
      " |      True\n",
      " |      >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}\n",
      " |      True\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(cls, *args: Optional[str], **kwargs: Optional[Any]) -> 'Row'\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from builtins.tuple:\n",
      " |  \n",
      " |  __add__(self, value, /)\n",
      " |      Return self+value.\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getnewargs__(self, /)\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __hash__(self, /)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __mul__(self, value, /)\n",
      " |      Return self*value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __rmul__(self, value, /)\n",
      " |      Return value*self.\n",
      " |  \n",
      " |  count(self, value, /)\n",
      " |      Return number of occurrences of value.\n",
      " |  \n",
      " |  index(self, value, start=0, stop=9223372036854775807, /)\n",
      " |      Return first index of value.\n",
      " |      \n",
      " |      Raises ValueError if the value is not present.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from builtins.tuple:\n",
      " |  \n",
      " |  __class_getitem__(...) from builtins.type\n",
      " |      See PEP 585\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Row) // Row(*args: Optional[str], **kwargs: Optional[Any]) -> 'Row'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Row('Alice', 11)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Varying list of arguments\n",
    "r=Row(\"Alice\",11)\n",
    "r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(name='Alice', age=11)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Varying keyword arguments\n",
    "row2 = Row(name='Alice',age=11)\n",
    "row2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alice'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row2.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alice'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row2['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert List of Lists into SparkDataframe using Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_lists =[[1,'Scott'],[2,'Donald'], [3,'Mickey'], [4,'Bunny']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| _1|    _2|\n",
      "+---+------+\n",
      "|  1| Scott|\n",
      "|  2|Donald|\n",
      "|  3|Mickey|\n",
      "|  4| Bunny|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(user_lists)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|user_id|user_name|\n",
      "+-------+---------+\n",
      "|      1|    Scott|\n",
      "|      2|   Donald|\n",
      "|      3|   Mickey|\n",
      "|      4|    Bunny|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(user_lists, 'user_id int,user_name string')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|user_id|user_name|\n",
      "+-------+---------+\n",
      "|      1|    Scott|\n",
      "|      2|   Donald|\n",
      "|      3|   Mickey|\n",
      "|      4|    Bunny|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#converted list of lists into list of rows\n",
    "# each list is of type row\n",
    "user_rows = [Row(*user)for user in user_list]\n",
    "user_rows\n",
    "df = spark.createDataFrame(user_rows, 'user_id int,user_name string')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(*args):\n",
    "    print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'hi')\n"
     ]
    }
   ],
   "source": [
    "dummy(1,'hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##when we have list and we want the list variables as varying arguments then we have to pass the value with * in the begning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 'Scott'],)\n"
     ]
    }
   ],
   "source": [
    "# Passing as one arguments and giving one argument\n",
    "user_details=[1,'Scott']\n",
    "dummy(user_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Scott')\n"
     ]
    }
   ],
   "source": [
    "# Passing as one arguments and giving varying argument\n",
    "user_details=[1,'Scott']\n",
    "dummy(*user_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##varying arguments *args and varying keyword arguments **kwargs\n",
    "Row(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list = [(1,'scott'),(2,'Donald'),(3,'Mickey'),(4,'Elvis')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| _1|    _2|\n",
      "+---+------+\n",
      "|  1| Scott|\n",
      "|  2|Donald|\n",
      "|  3|Mickey|\n",
      "|  4| Bunny|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(user_list)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|userid|username|\n",
      "+------+--------+\n",
      "|     1|   Scott|\n",
      "|     2|  Donald|\n",
      "|     3|  Mickey|\n",
      "|     4|   Bunny|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(user_list, 'userid int, username string')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Scott')\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "def dummy(*args):\n",
    "    print(args)\n",
    "dummy(*user_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_row = [Row(*user) for user in user_list]\n",
    "type(user_row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|userid|username|\n",
      "+------+--------+\n",
      "|     1|   Scott|\n",
      "|     2|  Donald|\n",
      "|     3|  Mickey|\n",
      "|     4|   Bunny|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(user_row,'userid int, username string')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list = [\n",
    "    {'user_details':1, 'user_first_name':'scott'},\n",
    "    {'user_details':2, 'user_first_name':'mickey'},\n",
    "    {'user_details':3, 'user_first_name':'donald'},\n",
    "    {'user_details':4, 'user_first_name':'bunny'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([2, 'mickey'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_details = user_list[0]\n",
    "## .values gives list\n",
    "user_details.values()\n",
    "Row(*user_details.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| _1|    _2|\n",
      "+---+------+\n",
      "|  1| scott|\n",
      "|  2|mickey|\n",
      "|  3|donald|\n",
      "|  4| bunny|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_rows= [Row(*users.values()) for users in user_list]\n",
    "df = spark.createDataFrame(user_rows)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+\n",
      "|user_details|user_first_name|\n",
      "+------------+---------------+\n",
      "|           1|          scott|\n",
      "|           2|         mickey|\n",
      "|           3|         donald|\n",
      "|           4|          bunny|\n",
      "+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## dict is converted to key word arguments by buiding the row object\n",
    "user_rows= [Row(**users) for users in user_list]\n",
    "df = spark.createDataFrame(user_rows)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(**kwargs):\n",
    "    print(kwargs)\n",
    "    print(len(kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_details': {'user_id': 1, 'user_name': 'scott'}}\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "user_details = {'user_id': 1, 'user_name':'scott'}\n",
    "dummy(user_details=user_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_id': 1, 'user_name': 'scott'}\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/24 01:54:48 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 297830 ms exceeds timeout 120000 ms\n",
      "22/08/24 01:54:48 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "dummy(**user_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dataframe with *args and **kwargs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pysparkcodingvenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "024d72e7fefe03842320fc063dcaae50b9599bc22df6b912d1daaeec955af601"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
