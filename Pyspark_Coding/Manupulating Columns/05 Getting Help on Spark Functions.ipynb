{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import date_format, col, lit, concat, concat_ws"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5430d9dc-601c-4959-905d-ec97b9be1c64"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["help(date_format)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a136d179-553b-4317-93c5-e62c5d621970"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Help on function date_format in module pyspark.sql.functions:\n\ndate_format(date, format)\n    Converts a date/timestamp/string to a value of string in the format specified by the date\n    format given by the second argument.\n    \n    A pattern could be for instance `dd.MM.yyyy` and could return a string like &#39;18.03.1993&#39;. All\n    pattern letters of `datetime pattern`_. can be used.\n    \n    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n    \n    .. versionadded:: 1.5.0\n    \n    Notes\n    -----\n    Whenever possible, use specialized functions like `year`.\n    \n    Examples\n    --------\n    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08&#39;,)], [&#39;dt&#39;])\n    &gt;&gt;&gt; df.select(date_format(&#39;dt&#39;, &#39;MM/dd/yyy&#39;).alias(&#39;date&#39;)).collect()\n    [Row(date=&#39;04/08/2015&#39;)]\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Help on function date_format in module pyspark.sql.functions:\n\ndate_format(date, format)\n    Converts a date/timestamp/string to a value of string in the format specified by the date\n    format given by the second argument.\n    \n    A pattern could be for instance `dd.MM.yyyy` and could return a string like &#39;18.03.1993&#39;. All\n    pattern letters of `datetime pattern`_. can be used.\n    \n    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n    \n    .. versionadded:: 1.5.0\n    \n    Notes\n    -----\n    Whenever possible, use specialized functions like `year`.\n    \n    Examples\n    --------\n    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08&#39;,)], [&#39;dt&#39;])\n    &gt;&gt;&gt; df.select(date_format(&#39;dt&#39;, &#39;MM/dd/yyy&#39;).alias(&#39;date&#39;)).collect()\n    [Row(date=&#39;04/08/2015&#39;)]\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["help(col)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"233a1dcb-b8b6-4bec-bb23-a95c66343afe"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Help on function col in module pyspark.sql.functions:\n\ncol(col)\n    Returns a :class:`~pyspark.sql.Column` based on the given column name.&#39;\n    Examples\n    --------\n    &gt;&gt;&gt; col(&#39;x&#39;)\n    Column&lt;&#39;x&#39;&gt;\n    &gt;&gt;&gt; column(&#39;x&#39;)\n    Column&lt;&#39;x&#39;&gt;\n    \n    .. versionadded:: 1.3\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Help on function col in module pyspark.sql.functions:\n\ncol(col)\n    Returns a :class:`~pyspark.sql.Column` based on the given column name.&#39;\n    Examples\n    --------\n    &gt;&gt;&gt; col(&#39;x&#39;)\n    Column&lt;&#39;x&#39;&gt;\n    &gt;&gt;&gt; column(&#39;x&#39;)\n    Column&lt;&#39;x&#39;&gt;\n    \n    .. versionadded:: 1.3\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["help(lit)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f1051c02-c19e-40ab-b6ae-451007804666"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Help on function lit in module pyspark.sql.functions:\n\nlit(col)\n    Creates a :class:`~pyspark.sql.Column` of literal value.\n    \n    .. versionadded:: 1.3.0\n    \n    Examples\n    --------\n    &gt;&gt;&gt; df.select(lit(5).alias(&#39;height&#39;)).withColumn(&#39;spark_user&#39;, lit(True)).take(1)\n    [Row(height=5, spark_user=True)]\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Help on function lit in module pyspark.sql.functions:\n\nlit(col)\n    Creates a :class:`~pyspark.sql.Column` of literal value.\n    \n    .. versionadded:: 1.3.0\n    \n    Examples\n    --------\n    &gt;&gt;&gt; df.select(lit(5).alias(&#39;height&#39;)).withColumn(&#39;spark_user&#39;, lit(True)).take(1)\n    [Row(height=5, spark_user=True)]\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["help(concat)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e9b5230-7eb7-469b-b1ca-394b32c3cd1a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Help on function concat in module pyspark.sql.functions:\n\nconcat(*cols)\n    Concatenates multiple input columns together into a single column.\n    The function works with strings, binary and compatible array columns.\n    \n    .. versionadded:: 1.5.0\n    \n    Examples\n    --------\n    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;abcd&#39;,&#39;123&#39;)], [&#39;s&#39;, &#39;d&#39;])\n    &gt;&gt;&gt; df.select(concat(df.s, df.d).alias(&#39;s&#39;)).collect()\n    [Row(s=&#39;abcd123&#39;)]\n    \n    &gt;&gt;&gt; df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])\n    &gt;&gt;&gt; df.select(concat(df.a, df.b, df.c).alias(&#34;arr&#34;)).collect()\n    [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Help on function concat in module pyspark.sql.functions:\n\nconcat(*cols)\n    Concatenates multiple input columns together into a single column.\n    The function works with strings, binary and compatible array columns.\n    \n    .. versionadded:: 1.5.0\n    \n    Examples\n    --------\n    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;abcd&#39;,&#39;123&#39;)], [&#39;s&#39;, &#39;d&#39;])\n    &gt;&gt;&gt; df.select(concat(df.s, df.d).alias(&#39;s&#39;)).collect()\n    [Row(s=&#39;abcd123&#39;)]\n    \n    &gt;&gt;&gt; df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])\n    &gt;&gt;&gt; df.select(concat(df.a, df.b, df.c).alias(&#34;arr&#34;)).collect()\n    [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["help(concat_ws)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"194a9422-a623-47c7-9c17-468c73f0ffb7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Help on function concat_ws in module pyspark.sql.functions:\n\nconcat_ws(sep, *cols)\n    Concatenates multiple input string columns together into a single string column,\n    using the given separator.\n    \n    .. versionadded:: 1.5.0\n    \n    Examples\n    --------\n    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;abcd&#39;,&#39;123&#39;)], [&#39;s&#39;, &#39;d&#39;])\n    &gt;&gt;&gt; df.select(concat_ws(&#39;-&#39;, df.s, df.d).alias(&#39;s&#39;)).collect()\n    [Row(s=&#39;abcd-123&#39;)]\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Help on function concat_ws in module pyspark.sql.functions:\n\nconcat_ws(sep, *cols)\n    Concatenates multiple input string columns together into a single string column,\n    using the given separator.\n    \n    .. versionadded:: 1.5.0\n    \n    Examples\n    --------\n    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;abcd&#39;,&#39;123&#39;)], [&#39;s&#39;, &#39;d&#39;])\n    &gt;&gt;&gt; df.select(concat_ws(&#39;-&#39;, df.s, df.d).alias(&#39;s&#39;)).collect()\n    [Row(s=&#39;abcd-123&#39;)]\n\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"05 Getting Help on Spark Functions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4062737143337662}},"nbformat":4,"nbformat_minor":0}
