{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pyspark' from '/Users/shubhangigupta/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/__init__.py'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/23 13:23:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/08/23 13:23:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/08/23 13:23:44 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('Word Count').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Json.load won't take multiple dcouments so for that we have to use splitlines to convert into list of multiple document into list of string \"\"\"{}{}{}{}\"\"\"---->splitlines()-->['{}','{}','{}','{}']\n",
    "['{}','{}','{}','{}'] --->(one by one) json.loads() -------> [{},{},{},{}]\n",
    "Row Function ---> Row(id=1, first_name='Frasco', last_name='Necolds', email='fnecolds0@vk.com', gender='Male', ip_address='243.67.63.34'), Row(id=2, first_name='Dulce', last_name='Santos', email='dsantos1@mashable.com', gender='Female', ip_address='60.30.246.227') Remember *args.values(),**kwargs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons3 = '''{\"id\":1,\"first_name\":\"Frasco\",\"last_name\":\"Necolds\",\"email\":\"fnecolds0@vk.com\",\"gender\":\"Male\",\"ip_address\":\"243.67.63.34\"}\n",
    "{\"id\":2,\"first_name\":\"Dulce\",\"last_name\":\"Santos\",\"email\":\"dsantos1@mashable.com\",\"gender\":\"Female\",\"ip_address\":\"60.30.246.227\"}\n",
    "{\"id\":3,\"first_name\":\"Prissie\",\"last_name\":\"Tebbett\",\"email\":\"ptebbett2@infoseek.co.jp\",\"gender\":\"Genderfluid\",\"ip_address\":\"22.21.162.56\"}\n",
    "{\"id\":4,\"first_name\":\"Schuyler\",\"last_name\":\"Coppledike\",\"email\":\"scoppledike3@gnu.org\",\"gender\":\"Agender\",\"ip_address\":\"120.35.186.161\"}\n",
    "{\"id\":5,\"first_name\":\"Leopold\",\"last_name\":\"Jarred\",\"email\":\"ljarred4@wp.com\",\"gender\":\"Agender\",\"ip_address\":\"30.119.34.4\"}\n",
    "{\"id\":6,\"first_name\":\"Joanna\",\"last_name\":\"Teager\",\"email\":\"jteager5@apache.org\",\"gender\":\"Bigender\",\"ip_address\":\"245.221.176.34\"}\n",
    "{\"id\":7,\"first_name\":\"Lion\",\"last_name\":\"Beere\",\"email\":\"lbeere6@bloomberg.com\",\"gender\":\"Polygender\",\"ip_address\":\"105.54.139.46\"}\n",
    "{\"id\":8,\"first_name\":\"Marabel\",\"last_name\":\"Wornum\",\"email\":\"mwornum7@posterous.com\",\"gender\":\"Polygender\",\"ip_address\":\"247.229.14.25\"}\n",
    "{\"id\":9,\"first_name\":\"Helenka\",\"last_name\":\"Mullender\",\"email\":\"hmullender8@cloudflare.com\",\"gender\":\"Non-binary\",\"ip_address\":\"133.216.118.88\"}\n",
    "{\"id\":10,\"first_name\":\"Christine\",\"last_name\":\"Swane\",\"email\":\"cswane9@shop-pro.jp\",\"gender\":\"Polygender\",\"ip_address\":\"86.16.210.164\"}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"id\":1,\"first_name\":\"Frasco\",\"last_name\":\"Necolds\",\"email\":\"fnecolds0@vk.com\",\"gender\":\"Male\",\"ip_address\":\"243.67.63.34\"}', '{\"id\":2,\"first_name\":\"Dulce\",\"last_name\":\"Santos\",\"email\":\"dsantos1@mashable.com\",\"gender\":\"Female\",\"ip_address\":\"60.30.246.227\"}', '{\"id\":3,\"first_name\":\"Prissie\",\"last_name\":\"Tebbett\",\"email\":\"ptebbett2@infoseek.co.jp\",\"gender\":\"Genderfluid\",\"ip_address\":\"22.21.162.56\"}', '{\"id\":4,\"first_name\":\"Schuyler\",\"last_name\":\"Coppledike\",\"email\":\"scoppledike3@gnu.org\",\"gender\":\"Agender\",\"ip_address\":\"120.35.186.161\"}', '{\"id\":5,\"first_name\":\"Leopold\",\"last_name\":\"Jarred\",\"email\":\"ljarred4@wp.com\",\"gender\":\"Agender\",\"ip_address\":\"30.119.34.4\"}', '{\"id\":6,\"first_name\":\"Joanna\",\"last_name\":\"Teager\",\"email\":\"jteager5@apache.org\",\"gender\":\"Bigender\",\"ip_address\":\"245.221.176.34\"}', '{\"id\":7,\"first_name\":\"Lion\",\"last_name\":\"Beere\",\"email\":\"lbeere6@bloomberg.com\",\"gender\":\"Polygender\",\"ip_address\":\"105.54.139.46\"}', '{\"id\":8,\"first_name\":\"Marabel\",\"last_name\":\"Wornum\",\"email\":\"mwornum7@posterous.com\",\"gender\":\"Polygender\",\"ip_address\":\"247.229.14.25\"}', '{\"id\":9,\"first_name\":\"Helenka\",\"last_name\":\"Mullender\",\"email\":\"hmullender8@cloudflare.com\",\"gender\":\"Non-binary\",\"ip_address\":\"133.216.118.88\"}', '{\"id\":10,\"first_name\":\"Christine\",\"last_name\":\"Swane\",\"email\":\"cswane9@shop-pro.jp\",\"gender\":\"Polygender\",\"ip_address\":\"86.16.210.164\"}']\n",
      "[{'id': 1, 'first_name': 'Frasco', 'last_name': 'Necolds', 'email': 'fnecolds0@vk.com', 'gender': 'Male', 'ip_address': '243.67.63.34'}, {'id': 2, 'first_name': 'Dulce', 'last_name': 'Santos', 'email': 'dsantos1@mashable.com', 'gender': 'Female', 'ip_address': '60.30.246.227'}, {'id': 3, 'first_name': 'Prissie', 'last_name': 'Tebbett', 'email': 'ptebbett2@infoseek.co.jp', 'gender': 'Genderfluid', 'ip_address': '22.21.162.56'}, {'id': 4, 'first_name': 'Schuyler', 'last_name': 'Coppledike', 'email': 'scoppledike3@gnu.org', 'gender': 'Agender', 'ip_address': '120.35.186.161'}, {'id': 5, 'first_name': 'Leopold', 'last_name': 'Jarred', 'email': 'ljarred4@wp.com', 'gender': 'Agender', 'ip_address': '30.119.34.4'}, {'id': 6, 'first_name': 'Joanna', 'last_name': 'Teager', 'email': 'jteager5@apache.org', 'gender': 'Bigender', 'ip_address': '245.221.176.34'}, {'id': 7, 'first_name': 'Lion', 'last_name': 'Beere', 'email': 'lbeere6@bloomberg.com', 'gender': 'Polygender', 'ip_address': '105.54.139.46'}, {'id': 8, 'first_name': 'Marabel', 'last_name': 'Wornum', 'email': 'mwornum7@posterous.com', 'gender': 'Polygender', 'ip_address': '247.229.14.25'}, {'id': 9, 'first_name': 'Helenka', 'last_name': 'Mullender', 'email': 'hmullender8@cloudflare.com', 'gender': 'Non-binary', 'ip_address': '133.216.118.88'}, {'id': 10, 'first_name': 'Christine', 'last_name': 'Swane', 'email': 'cswane9@shop-pro.jp', 'gender': 'Polygender', 'ip_address': '86.16.210.164'}]\n",
      "[Row(id=1, first_name='Frasco', last_name='Necolds', email='fnecolds0@vk.com', gender='Male', ip_address='243.67.63.34'), Row(id=2, first_name='Dulce', last_name='Santos', email='dsantos1@mashable.com', gender='Female', ip_address='60.30.246.227'), Row(id=3, first_name='Prissie', last_name='Tebbett', email='ptebbett2@infoseek.co.jp', gender='Genderfluid', ip_address='22.21.162.56'), Row(id=4, first_name='Schuyler', last_name='Coppledike', email='scoppledike3@gnu.org', gender='Agender', ip_address='120.35.186.161'), Row(id=5, first_name='Leopold', last_name='Jarred', email='ljarred4@wp.com', gender='Agender', ip_address='30.119.34.4'), Row(id=6, first_name='Joanna', last_name='Teager', email='jteager5@apache.org', gender='Bigender', ip_address='245.221.176.34'), Row(id=7, first_name='Lion', last_name='Beere', email='lbeere6@bloomberg.com', gender='Polygender', ip_address='105.54.139.46'), Row(id=8, first_name='Marabel', last_name='Wornum', email='mwornum7@posterous.com', gender='Polygender', ip_address='247.229.14.25'), Row(id=9, first_name='Helenka', last_name='Mullender', email='hmullender8@cloudflare.com', gender='Non-binary', ip_address='133.216.118.88'), Row(id=10, first_name='Christine', last_name='Swane', email='cswane9@shop-pro.jp', gender='Polygender', ip_address='86.16.210.164')]\n",
      "+---+----------+----------+--------------------+-----------+--------------+\n",
      "| id|first_name| last_name|               email|     gender|    ip_address|\n",
      "+---+----------+----------+--------------------+-----------+--------------+\n",
      "|  1|    Frasco|   Necolds|    fnecolds0@vk.com|       Male|  243.67.63.34|\n",
      "|  2|     Dulce|    Santos|dsantos1@mashable...|     Female| 60.30.246.227|\n",
      "|  3|   Prissie|   Tebbett|ptebbett2@infosee...|Genderfluid|  22.21.162.56|\n",
      "|  4|  Schuyler|Coppledike|scoppledike3@gnu.org|    Agender|120.35.186.161|\n",
      "|  5|   Leopold|    Jarred|     ljarred4@wp.com|    Agender|   30.119.34.4|\n",
      "|  6|    Joanna|    Teager| jteager5@apache.org|   Bigender|245.221.176.34|\n",
      "|  7|      Lion|     Beere|lbeere6@bloomberg...| Polygender| 105.54.139.46|\n",
      "|  8|   Marabel|    Wornum|mwornum7@posterou...| Polygender| 247.229.14.25|\n",
      "|  9|   Helenka| Mullender|hmullender8@cloud...| Non-binary|133.216.118.88|\n",
      "| 10| Christine|     Swane| cswane9@shop-pro.jp| Polygender| 86.16.210.164|\n",
      "+---+----------+----------+--------------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import Row\n",
    "persons4 = persons3.splitlines()\n",
    "print(persons4)\n",
    "import json\n",
    "json_data = [json.loads(i) for i in persons4]\n",
    "print(json_data)\n",
    "row_json_data = [Row(**data) for data in json_data]\n",
    "print(row_json_data)\n",
    "df = spark.createDataFrame(row_json_data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "person1=\"\"\"{\n",
    "  \"firstName\": \"John\",\n",
    "  \"lastName\": \"Smith\",\n",
    "  \"isAlive\": true,\n",
    "  \"age\": 27,\n",
    "  \"address\": {\n",
    "    \"streetAddress\": \"21 2nd Street\",\n",
    "    \"city\": \"New York\",\n",
    "    \"state\": \"NY\",\n",
    "    \"postalCode\": \"10021-3100\"\n",
    "  },\n",
    "  \"phoneNumbers\": [\n",
    "    {\n",
    "      \"type\": \"home\",\n",
    "      \"number\": \"212 555-1234\"\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"office\",\n",
    "      \"number\": \"646 555-4567\"\n",
    "    }\n",
    "  ],\n",
    "  \"children\": [],\n",
    "  \"spouse\": null\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(firstName='John', lastName='Smith', isAlive=True, age=27, address={'streetAddress': '21 2nd Street', 'city': 'New York', 'state': 'NY', 'postalCode': '10021-3100'}, phoneNumbers=[{'type': 'home', 'number': '212 555-1234'}, {'type': 'office', 'number': '646 555-4567'}], children=[], spouse=None)]\n",
      "+---------+--------+-------+---+--------------------+--------------------+--------+------+\n",
      "|firstName|lastName|isAlive|age|             address|        phoneNumbers|children|spouse|\n",
      "+---------+--------+-------+---+--------------------+--------------------+--------+------+\n",
      "|     John|   Smith|   true| 27|{state=NY, city=N...|[{type -> home, n...|      []|  null|\n",
      "+---------+--------+-------+---+--------------------+--------------------+--------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/24 01:07:31 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 284832 ms exceeds timeout 120000 ms\n",
      "22/08/24 01:07:31 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StringType\n",
    "import json\n",
    "\n",
    "jsonDataList = []\n",
    "person=json.loads(person1)\n",
    "list_p=[person]\n",
    "from pyspark.sql.types import StructField, StructType, StringType, MapType,IntegerType, ArrayType, BooleanType\n",
    "schema = StructType([\n",
    "    StructField('firstName', StringType(), True),\n",
    "    StructField('lastName', StringType(), True) ,\n",
    "    StructField('isAlive', StringType(), True),\n",
    "    StructField('age', IntegerType(), True),\n",
    "    StructField('address', StringType(),True),\n",
    "    StructField('phoneNumbers', ArrayType(MapType(StringType(),StringType())), True),\n",
    "    StructField(\"children\", ArrayType(StringType()), True),\n",
    "    StructField('spouse', StringType(), True) \n",
    "    ])\n",
    "print([Row(**json_val) for json_val in list_p])\n",
    "df = spark.createDataFrame([Row(**json_val) for json_val in list_p], schema=schema)\n",
    "df.show()\n",
    "\n",
    "\n",
    "# json_data = json.loads(person1)\n",
    "# print(json_data)\n",
    "\n",
    "\n",
    "\n",
    "# df = spark.createDataFrame(json_data, schema=schema).show()   \n",
    "# df.show()   \n",
    "\n",
    "\n",
    "\n",
    "# pd.DataFrame(json_data.items())\n",
    "# person2 = json.loads(person1)\n",
    "#df = pd.json_normalize(person2, record_path =['phoneNumbers'])\n",
    "\n",
    "# s = pd.DataFrame(list(person2.items()))\n",
    "# s\n",
    "\n",
    "# #spark.createDataFrame(pd.DataFrame(list(person2.items()))).show()\n",
    "# spark.createDataFrame(person2).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [{'points': 50, 'time': '5:00', 'month': \"janurary\"}, \n",
    "         {'points': 25, 'time': '6:00', 'month': \"february\"}, \n",
    "         {'points':90, 'time': '9:00', 'month': \"january\"}, \n",
    "         {'points':20, 'month': \"june\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([StructField(\"Customer\", StringType(), True)\\\n",
    "                   ,StructField(\"month\", StringType(), True)\\\n",
    "                   ,StructField(\"points\", IntegerType(), True)\\\n",
    "                    ,StructField(\"time\", StringType(), True)\n",
    "                   ])\n",
    "                   \n",
    "                   \n",
    "                   \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points</th>\n",
       "      <th>time</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>points_h1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50.0</td>\n",
       "      <td>5:00</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.0</td>\n",
       "      <td>6:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>february</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.0</td>\n",
       "      <td>9:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>january</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>june</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   points  time    year     month  points_h1\n",
       "0    50.0  5:00  2010.0       NaN        NaN\n",
       "1    25.0  6:00     NaN  february        NaN\n",
       "2    90.0  9:00     NaN   january        NaN\n",
       "3     NaN   NaN     NaN      june       20.0"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+----+\n",
      "|Customer|   month|points|time|\n",
      "+--------+--------+------+----+\n",
      "|    null|    null|    50|5:00|\n",
      "|    null|february|    25|6:00|\n",
      "|    null| january|    90|9:00|\n",
      "|    null|    june|  null|null|\n",
      "+--------+--------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.createDataFrame(d, schema=schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{\"Category\": 'Category A', \"ID\": 1, \"Value\": 12.40},\n",
    "        {\"Category\": 'Category B', \"ID\": 2, \"Value\": 30.10},\n",
    "        {\"Category\": 'Category C', \"ID\": 3, \"Value\": 100.01}\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points</th>\n",
       "      <th>time</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>5:00</td>\n",
       "      <td>janurary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>6:00</td>\n",
       "      <td>february</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90</td>\n",
       "      <td>9:00</td>\n",
       "      <td>january</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>june</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   points  time     month\n",
       "0      50  5:00  janurary\n",
       "1      25  6:00  february\n",
       "2      90  9:00   january\n",
       "3      20   NaN      june"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data1)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "field time: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/shubhangigupta/Desktop/Pyspark_Coding/Playingwithdict.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/shubhangigupta/Desktop/Pyspark_Coding/Playingwithdict.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m spark\u001b[39m.\u001b[39;49mcreateDataFrame(df)\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/sql/session.py:891\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    888\u001b[0m     has_pandas \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[39mif\u001b[39;00m has_pandas \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pandas\u001b[39m.\u001b[39mDataFrame):\n\u001b[1;32m    890\u001b[0m     \u001b[39m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m--> 891\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(SparkSession, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mcreateDataFrame(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m    893\u001b[0m     )\n\u001b[1;32m    894\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_dataframe(\n\u001b[1;32m    895\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    896\u001b[0m )\n",
      "File \u001b[0;32m~/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:437\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[1;32m    436\u001b[0m converted_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[0;32m--> 437\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[0;32m~/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/sql/session.py:936\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    934\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_createFromRDD(data\u001b[39m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m    935\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_createFromLocal(\u001b[39mmap\u001b[39;49m(prepare, data), schema)\n\u001b[1;32m    937\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    938\u001b[0m jrdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mSerDeUtil\u001b[39m.\u001b[39mtoJavaArray(rdd\u001b[39m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m~/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/sql/session.py:631\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    628\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(data)\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 631\u001b[0m     struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inferSchemaFromList(data, names\u001b[39m=\u001b[39;49mschema)\n\u001b[1;32m    632\u001b[0m     converter \u001b[39m=\u001b[39m _create_converter(struct)\n\u001b[1;32m    633\u001b[0m     tupled_data: Iterable[Tuple] \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(converter, data)\n",
      "File \u001b[0;32m~/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/sql/session.py:517\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    515\u001b[0m infer_dict_as_struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jconf\u001b[39m.\u001b[39minferDictAsStruct()\n\u001b[1;32m    516\u001b[0m prefer_timestamp_ntz \u001b[39m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[0;32m--> 517\u001b[0m schema \u001b[39m=\u001b[39m reduce(\n\u001b[1;32m    518\u001b[0m     _merge_type,\n\u001b[1;32m    519\u001b[0m     (_infer_schema(row, names, infer_dict_as_struct, prefer_timestamp_ntz) \u001b[39mfor\u001b[39;49;00m row \u001b[39min\u001b[39;49;00m data),\n\u001b[1;32m    520\u001b[0m )\n\u001b[1;32m    521\u001b[0m \u001b[39mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    522\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSome of types cannot be determined after inferring\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/sql/types.py:1383\u001b[0m, in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, StructType):\n\u001b[1;32m   1382\u001b[0m     nfs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m((f\u001b[39m.\u001b[39mname, f\u001b[39m.\u001b[39mdataType) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m cast(StructType, b)\u001b[39m.\u001b[39mfields)\n\u001b[0;32m-> 1383\u001b[0m     fields \u001b[39m=\u001b[39m [\n\u001b[1;32m   1384\u001b[0m         StructField(\n\u001b[1;32m   1385\u001b[0m             f\u001b[39m.\u001b[39mname, _merge_type(f\u001b[39m.\u001b[39mdataType, nfs\u001b[39m.\u001b[39mget(f\u001b[39m.\u001b[39mname, NullType()), name\u001b[39m=\u001b[39mnew_name(f\u001b[39m.\u001b[39mname))\n\u001b[1;32m   1386\u001b[0m         )\n\u001b[1;32m   1387\u001b[0m         \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m a\u001b[39m.\u001b[39mfields\n\u001b[1;32m   1388\u001b[0m     ]\n\u001b[1;32m   1389\u001b[0m     names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m([f\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fields])\n\u001b[1;32m   1390\u001b[0m     \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m nfs:\n",
      "File \u001b[0;32m~/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/sql/types.py:1385\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, StructType):\n\u001b[1;32m   1382\u001b[0m     nfs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m((f\u001b[39m.\u001b[39mname, f\u001b[39m.\u001b[39mdataType) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m cast(StructType, b)\u001b[39m.\u001b[39mfields)\n\u001b[1;32m   1383\u001b[0m     fields \u001b[39m=\u001b[39m [\n\u001b[1;32m   1384\u001b[0m         StructField(\n\u001b[0;32m-> 1385\u001b[0m             f\u001b[39m.\u001b[39mname, _merge_type(f\u001b[39m.\u001b[39;49mdataType, nfs\u001b[39m.\u001b[39;49mget(f\u001b[39m.\u001b[39;49mname, NullType()), name\u001b[39m=\u001b[39;49mnew_name(f\u001b[39m.\u001b[39;49mname))\n\u001b[1;32m   1386\u001b[0m         )\n\u001b[1;32m   1387\u001b[0m         \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m a\u001b[39m.\u001b[39mfields\n\u001b[1;32m   1388\u001b[0m     ]\n\u001b[1;32m   1389\u001b[0m     names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m([f\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fields])\n\u001b[1;32m   1390\u001b[0m     \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m nfs:\n",
      "File \u001b[0;32m~/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/sql/types.py:1378\u001b[0m, in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1375\u001b[0m     \u001b[39mreturn\u001b[39;00m b\n\u001b[1;32m   1376\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(a) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mtype\u001b[39m(b):\n\u001b[1;32m   1377\u001b[0m     \u001b[39m# TODO: type cast (such as int -> long)\u001b[39;00m\n\u001b[0;32m-> 1378\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(new_msg(\u001b[39m\"\u001b[39m\u001b[39mCan not merge type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mtype\u001b[39m(a), \u001b[39mtype\u001b[39m(b))))\n\u001b[1;32m   1380\u001b[0m \u001b[39m# same type\u001b[39;00m\n\u001b[1;32m   1381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, StructType):\n",
      "\u001b[0;31mTypeError\u001b[0m: field time: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(df).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pysparkcodingvenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "024d72e7fefe03842320fc063dcaae50b9599bc22df6b912d1daaeec955af601"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
