{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pyspark' from '/Users/shubhangigupta/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/24 18:40:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/08/24 18:40:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('Word Count').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.0.10-py2.py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.1/242.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.0.10\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Name Contact Person / Hiring Manager  \\\n",
      "0                True Car                             NaN   \n",
      "1          Allison Decker                             NaN   \n",
      "2                  Oracle                             NaN   \n",
      "3                    Uber                             NaN   \n",
      "4               Mc Kinsey                             NaN   \n",
      "5           Meta (Direct)                             NaN   \n",
      "6         Meta (Contract)                             NaN   \n",
      "7        Virgin Hyperloop                             NaN   \n",
      "8                  Google                             NaN   \n",
      "9                     AXS                             NaN   \n",
      "10                 Disney                             NaN   \n",
      "11     Ingram Micro Cloud                             NaN   \n",
      "12    Data engineer soCal          Paige Raszl / Linkedin   \n",
      "13        Elizabeth Ryder                        LinkedIn   \n",
      "14  Apex Systems / Kaiser                            mail   \n",
      "15   J&J Data Engineering                            mail   \n",
      "16                 SLolom                            mail   \n",
      "17                 Kaiser                            Mail   \n",
      "\n",
      "                                         Email/ Phone  \\\n",
      "0                                                 NaN   \n",
      "1                                                 NaN   \n",
      "2                                                 NaN   \n",
      "3                                                 NaN   \n",
      "4                                                 NaN   \n",
      "5                                                 NaN   \n",
      "6                                                 NaN   \n",
      "7                                                 NaN   \n",
      "8                                                 NaN   \n",
      "9                                                 NaN   \n",
      "10                                                NaN   \n",
      "11                                                NaN   \n",
      "12                                   praszl@judge.com   \n",
      "13                                                NaN   \n",
      "14          \\nDaisy Joseph <dajoseph@apexsystems.com>   \n",
      "15  Ankur Dogra/ adogra1@its.jnj.com |        (267...   \n",
      "16           \\nAaron Benson <aaron.benson@slalom.com>   \n",
      "17                                       Daisy joseph   \n",
      "\n",
      "                                   Status  \\\n",
      "0                 Followed up on 19th aug   \n",
      "1   Give dates waiting for quantum black    \n",
      "2                                     NaN   \n",
      "3                         8/26 Uber exam    \n",
      "4                  Follow up sent on 8/22   \n",
      "5                                     NaN   \n",
      "6                                     NaN   \n",
      "7                          Take follow up   \n",
      "8                                     NaN   \n",
      "9                                     NaN   \n",
      "10                                    NaN   \n",
      "11                                    NaN   \n",
      "12                                    NaN   \n",
      "13                 Follow up sent on 8/19   \n",
      "14                                    NaN   \n",
      "15                             email 8/20   \n",
      "16                                    NaN   \n",
      "17                Follow up sent on 22/22   \n",
      "\n",
      "                                  Introductory Call  \\\n",
      "0                                               NaN   \n",
      "1                                               NaN   \n",
      "2                                    8/24 11 AM PDT   \n",
      "3                                               NaN   \n",
      "4                                               NaN   \n",
      "5                                               NaN   \n",
      "6                                     Contract 8/29   \n",
      "7                                               NaN   \n",
      "8                               2022-08-26 10:30:00   \n",
      "9                                  25 Aug 3 to 3:30   \n",
      "10                                              NaN   \n",
      "11                                   25th Aug 11:30   \n",
      "12                                              NaN   \n",
      "13                                              NaN   \n",
      "14  Introductory Call Done, Give time for 2st round   \n",
      "15                              contact on 29th Aug   \n",
      "16                                              NaN   \n",
      "17                                              NaN   \n",
      "\n",
      "                  1st interview  2nd interview  \n",
      "0                           NaN            NaN  \n",
      "1                           NaN            NaN  \n",
      "2                           NaN            NaN  \n",
      "3                           NaN            NaN  \n",
      "4   6,7,8 Sep (To Be Scheduled)            NaN  \n",
      "5                           NaN            NaN  \n",
      "6                           NaN            NaN  \n",
      "7                           NaN            NaN  \n",
      "8                           NaN            NaN  \n",
      "9                           NaN            NaN  \n",
      "10                          NaN            NaN  \n",
      "11                          NaN            NaN  \n",
      "12                          NaN            NaN  \n",
      "13                          NaN            NaN  \n",
      "14                          NaN            NaN  \n",
      "15                          NaN            NaN  \n",
      "16                          NaN            NaN  \n",
      "17                          NaN            NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('/Users/shubhangigupta/Desktop/InterviewSchedule.xlsx')\n",
    "df1 = pd.DataFrame(df)\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "field Status: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/shubhangigupta/Desktop/Pyspark_Coding/P1. Createdataframefrompandas.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/shubhangigupta/Desktop/Pyspark_Coding/P1.%20Createdataframefrompandas.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mcreateDataFrame(df1)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shubhangigupta/Desktop/Pyspark_Coding/P1.%20Createdataframefrompandas.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(df)\n",
      "File \u001b[0;32m~/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/sql/session.py:891\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    888\u001b[0m     has_pandas \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[39mif\u001b[39;00m has_pandas \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pandas\u001b[39m.\u001b[39mDataFrame):\n\u001b[1;32m    890\u001b[0m     \u001b[39m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m--> 891\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(SparkSession, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mcreateDataFrame(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m    893\u001b[0m     )\n\u001b[1;32m    894\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_dataframe(\n\u001b[1;32m    895\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    896\u001b[0m )\n",
      "File \u001b[0;32m~/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:437\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[1;32m    436\u001b[0m converted_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[0;32m--> 437\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[0;32m~/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/sql/session.py:936\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    934\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_createFromRDD(data\u001b[39m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m    935\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_createFromLocal(\u001b[39mmap\u001b[39;49m(prepare, data), schema)\n\u001b[1;32m    937\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    938\u001b[0m jrdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mSerDeUtil\u001b[39m.\u001b[39mtoJavaArray(rdd\u001b[39m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m~/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/sql/session.py:631\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    628\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(data)\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 631\u001b[0m     struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inferSchemaFromList(data, names\u001b[39m=\u001b[39;49mschema)\n\u001b[1;32m    632\u001b[0m     converter \u001b[39m=\u001b[39m _create_converter(struct)\n\u001b[1;32m    633\u001b[0m     tupled_data: Iterable[Tuple] \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(converter, data)\n",
      "File \u001b[0;32m~/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/sql/session.py:517\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    515\u001b[0m infer_dict_as_struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jconf\u001b[39m.\u001b[39minferDictAsStruct()\n\u001b[1;32m    516\u001b[0m prefer_timestamp_ntz \u001b[39m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[0;32m--> 517\u001b[0m schema \u001b[39m=\u001b[39m reduce(\n\u001b[1;32m    518\u001b[0m     _merge_type,\n\u001b[1;32m    519\u001b[0m     (_infer_schema(row, names, infer_dict_as_struct, prefer_timestamp_ntz) \u001b[39mfor\u001b[39;49;00m row \u001b[39min\u001b[39;49;00m data),\n\u001b[1;32m    520\u001b[0m )\n\u001b[1;32m    521\u001b[0m \u001b[39mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    522\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSome of types cannot be determined after inferring\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/sql/types.py:1383\u001b[0m, in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, StructType):\n\u001b[1;32m   1382\u001b[0m     nfs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m((f\u001b[39m.\u001b[39mname, f\u001b[39m.\u001b[39mdataType) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m cast(StructType, b)\u001b[39m.\u001b[39mfields)\n\u001b[0;32m-> 1383\u001b[0m     fields \u001b[39m=\u001b[39m [\n\u001b[1;32m   1384\u001b[0m         StructField(\n\u001b[1;32m   1385\u001b[0m             f\u001b[39m.\u001b[39mname, _merge_type(f\u001b[39m.\u001b[39mdataType, nfs\u001b[39m.\u001b[39mget(f\u001b[39m.\u001b[39mname, NullType()), name\u001b[39m=\u001b[39mnew_name(f\u001b[39m.\u001b[39mname))\n\u001b[1;32m   1386\u001b[0m         )\n\u001b[1;32m   1387\u001b[0m         \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m a\u001b[39m.\u001b[39mfields\n\u001b[1;32m   1388\u001b[0m     ]\n\u001b[1;32m   1389\u001b[0m     names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m([f\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fields])\n\u001b[1;32m   1390\u001b[0m     \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m nfs:\n",
      "File \u001b[0;32m~/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/sql/types.py:1385\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, StructType):\n\u001b[1;32m   1382\u001b[0m     nfs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m((f\u001b[39m.\u001b[39mname, f\u001b[39m.\u001b[39mdataType) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m cast(StructType, b)\u001b[39m.\u001b[39mfields)\n\u001b[1;32m   1383\u001b[0m     fields \u001b[39m=\u001b[39m [\n\u001b[1;32m   1384\u001b[0m         StructField(\n\u001b[0;32m-> 1385\u001b[0m             f\u001b[39m.\u001b[39mname, _merge_type(f\u001b[39m.\u001b[39;49mdataType, nfs\u001b[39m.\u001b[39;49mget(f\u001b[39m.\u001b[39;49mname, NullType()), name\u001b[39m=\u001b[39;49mnew_name(f\u001b[39m.\u001b[39;49mname))\n\u001b[1;32m   1386\u001b[0m         )\n\u001b[1;32m   1387\u001b[0m         \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m a\u001b[39m.\u001b[39mfields\n\u001b[1;32m   1388\u001b[0m     ]\n\u001b[1;32m   1389\u001b[0m     names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m([f\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fields])\n\u001b[1;32m   1390\u001b[0m     \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m nfs:\n",
      "File \u001b[0;32m~/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/sql/types.py:1378\u001b[0m, in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1375\u001b[0m     \u001b[39mreturn\u001b[39;00m b\n\u001b[1;32m   1376\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(a) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mtype\u001b[39m(b):\n\u001b[1;32m   1377\u001b[0m     \u001b[39m# TODO: type cast (such as int -> long)\u001b[39;00m\n\u001b[0;32m-> 1378\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(new_msg(\u001b[39m\"\u001b[39m\u001b[39mCan not merge type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mtype\u001b[39m(a), \u001b[39mtype\u001b[39m(b))))\n\u001b[1;32m   1380\u001b[0m \u001b[39m# same type\u001b[39;00m\n\u001b[1;32m   1381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, StructType):\n",
      "\u001b[0;31mTypeError\u001b[0m: field Status: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>"
     ]
    }
   ],
   "source": [
    "## inferschema\n",
    "df = spark.createDataFrame(df1)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pysparkcodingvenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "024d72e7fefe03842320fc063dcaae50b9599bc22df6b912d1daaeec955af601"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
