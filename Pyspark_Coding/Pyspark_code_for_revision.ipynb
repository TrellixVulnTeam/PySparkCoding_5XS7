{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "269abbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.6\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6cff9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (3.3.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pyspark) (0.10.9.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad1c907",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb83653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://bootstrap.pypa.io/get-pip.py | python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb46572",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c9ae837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in ./opt/anaconda3/lib/python3.9/site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3489e5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6a4f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SPARK_HOME\"] = \"/Users/shubhangigupta/hadoop/spark-3.3.0-bin-hadoop3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15b0ad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a3451fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pyspark' from '/Users/shubhangigupta/hadoop/spark-3.3.0-bin-hadoop3/python/pyspark/__init__.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dca45263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a3f8ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/13 13:45:56 WARN Utils: Your hostname, Shubhangis-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 172.20.5.4 instead (on interface en0)\n",
      "22/08/13 13:45:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/13 13:45:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName('Word Count').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8393aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02945b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv(\"/Users/shubhangigupta/Desktop/data_bigdatahadoop\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b077598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.select('passenger_count', 'trip_distance', 'tip_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea015b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9c64c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_1 = df2.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1353a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80efdfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_2 = df2.dropDuplicates()\n",
    "df3_2.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec5020",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.dropDuplicates()\n",
    "# Count the number of records in d3\n",
    "df3.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13263e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.filter(df3.passenger_count == 2).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a74a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.describe('trip_distance').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94816aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "f1 = spark.sql('show functions')\n",
    "print(type(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbd51fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv(\"/Users/shubhangigupta/Desktop/empdata.csv\", header=True, inferSchema=True)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7717ff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.withColumn('day', date_add(current_date(),2))\n",
    "df2.write.partitionBy('day').mode('append').saveAsTable('callbydate')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00853f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "conn = mysql.connector.connect(user='root', database='strata',\n",
    "                               password='CpZhsPUwx3',\n",
    "                               host=\"localhost\",\n",
    "                               port=3306)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccad1c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72853aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = conn.cursor()\n",
    "query = \"SELECT sender FROM facebook_messages_sent\"\n",
    "# Create a pandas dataframe\n",
    "pdf = pd.read_sql(query, con=conn)\n",
    "conn.close()\n",
    "\n",
    "# Convert Pandas dataframe to spark DataFrame\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaade4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config(\"spark.jars\", \"/Users/shubhangigupta/spark-3.2.1-bin-hadoop3.2/jars/mysql-connector-java-8.0.29.jar\") \\\n",
    "    .master(\"local\").appName(\"PySpark_MySQL_test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774fd6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df22aced",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_mysql = spark.read.format(\"jdbc\").options(\n",
    "    url=\"jdbc:mysql://localhost:3306/strata\",\n",
    "    driver = \"com.mysql.jdbc.Driver\",\n",
    "    dbtable = \"facebook_messages_sent\",\n",
    "    user=\"root\",\n",
    "    password=\"CpZhsPUwx3\").load()\n",
    "\n",
    "dataframe_mysql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29231be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('/Users/shubhangigupta/Desktop/files/amazon_purchases.csv', header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d452774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the 3-month rolling average of total revenue from purchases given a table with users, their purchase amount, and date purchased. Do not include returns which are represented by negative purchase values. Output the year-month (YYYY-MM) and 3-month rolling average of revenue, sorted from earliest month to latest month.\n",
    "#A 3-month rolling average is defined by calculating the average total revenue from all user purchases for the current month and previous two months. The first two months will not be a true 3-month rolling average since we are not given data from last year. Assume each month has at least one purchase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b28bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a74fbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c49b1905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cast: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- date_added: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- director: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- listed_in: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- release_year: string (nullable = true)\n",
      " |-- show_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df = spark.read.option(\"multiline\",\"true\").json(\"/Users/shubhangigupta/Desktop/log_filedndjson_file/data.json\")\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b58859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType , StructField, IntegerType, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "79cd921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sampleSchema = StructType([\n",
    "    StructField(\"cast\",StringType(),True), \n",
    "    StructField(\"ct\",StringType(),True), \n",
    "    StructField(\"date_added\", StringType(), True), \n",
    "    StructField(\"description\", StringType(), True), \n",
    "    StructField(\"director\", StringType(), True), \n",
    "    StructField(\"duration\", StringType(), True), \n",
    "    StructField(\"listed_in\",StringType(),True), \n",
    "    StructField(\"rating\",StringType(),True), \n",
    "    StructField(\"release_year\", StringType(), True), \n",
    "    StructField(\"show_id\", StringType(), True), \n",
    "    StructField(\"title\", StringType(), True), \n",
    "    StructField(\"type\", StringType(), True)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b5d534bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----------------+--------------------+--------------------+----------+--------------------+------+------------+-------+--------------------+-------+\n",
      "|                cast|  ct|       date_added|         description|            director|  duration|           listed_in|rating|release_year|show_id|               title|   type|\n",
      "+--------------------+----+-----------------+--------------------+--------------------+----------+--------------------+------+------------+-------+--------------------+-------+\n",
      "|Chris Diamantopou...|null|November 26, 2021|Join Mickey and t...|Alonso Ramirez Ra...|    23 min|   Animation, Family|  TV-G|        2016|     s1|Duck the Halls: A...|  Movie|\n",
      "|Jim Varney, Noell...|null|November 26, 2021|Santa Claus passe...|         John Cherry|    91 min|              Comedy|    PG|        1988|     s2|Ernest Saves Chri...|  Movie|\n",
      "|Raymond Albert Ro...|null|November 26, 2021|Sid the Sloth is ...|        Karen Disher|    23 min|Animation, Comedy...|  TV-G|        2011|     s3|Ice Age: A Mammot...|  Movie|\n",
      "|Darren Criss, Ada...|null|November 26, 2021|This is real life...|     Hamish Hamilton|    41 min|             Musical| TV-PG|        2021|     s4|The Queen Family ...|  Movie|\n",
      "|John Lennon, Paul...|null|November 25, 2021|A three-part docu...|                    |  1 Season|Docuseries, Histo...|      |        2021|     s5|The Beatles: Get ...|TV Show|\n",
      "|Jacques Yves Cous...|null|November 24, 2021|An inside look at...|          Liz Garbus|    94 min|Biographical, Doc...| PG-13|        2021|     s6|   Becoming Cousteau|  Movie|\n",
      "|Jeremy Renner, Ha...|null|November 24, 2021|Clint Barton/Hawk...|                    |  1 Season|Action-Adventure,...| TV-14|        2021|     s7|             Hawkeye|TV Show|\n",
      "|Gary Muehlberger,...|null|November 24, 2021|Residents of Port...|                    | 2 Seasons|Docuseries, Reali...| TV-14|        2015|     s8|Port Protection A...|TV Show|\n",
      "|Dr. Ray Ball, Dr....|null|November 24, 2021|A day in the life...|                    | 2 Seasons|Animals & Nature,...| TV-PG|        2019|     s9|Secrets of the Zo...|TV Show|\n",
      "|Steve Whitmire, D...|null|November 19, 2021|Celebrate the hol...|    Kirk R. Thatcher|    45 min|Comedy, Family, M...|     G|        2008|    s10|A Muppets Christm...|  Movie|\n",
      "|Don Hahn, Kathryn...|null|November 19, 2021|Explore the treas...|          John Gleim|    59 min|         Documentary| TV-PG|        2020|    s11|Adventure Thru th...|  Movie|\n",
      "|                    |null|November 19, 2021|Check out Daveed ...|                    |     4 min|Comedy, Family, M...|  TV-G|        2020|    s12|  Puppy for Hanukkah|  Movie|\n",
      "|Stacy Keach, John...|null|November 19, 2021|A groundbreaking ...|       Leslie Iwerks|    91 min| Documentary, Family|     G|        2007|    s13|     The Pixar Story|  Movie|\n",
      "|Dr. Michelle Oakl...|null|November 17, 2021|Meet Dr. Michelle...|                    |10 Seasons|Action-Adventure,...| TV-PG|        2013|    s14|Dr. Oakley, Yukon...|TV Show|\n",
      "|   Michael B. Jordan|null|November 12, 2021|Epic, grand portr...|                    |     2 min|    Animals & Nature| TV-PG|        2021|    s15|America the Beaut...|  Movie|\n",
      "|                    |null|November 12, 2021|“Baymax!” premier...|                    |     1 min|           Animation|  TV-G|        2021|    s16|             Baymax!|  Movie|\n",
      "|Jack Dylan Grazer...|null|November 12, 2021|Fun-loving sea mo...|      McKenna Harris|     8 min|Animation, Comedy...| TV-PG|        2021|    s17|        Ciao Alberto|  Movie|\n",
      "|Mia Jenness, Alys...|null|November 12, 2021|Nancy makes every...|                    | 3 Seasons|     Animation, Kids| TV-PG|        2018|    s18|  Disney Fancy Nancy|TV Show|\n",
      "|Carolina Domenech...|null|November 12, 2021|Allegra is ready ...|                    |  1 Season|Comedy, Coming of...| TV-PG|        2021|    s19|  Disney Intertwined|TV Show|\n",
      "|Amy Adams, Patric...|null|November 12, 2021|An animated princ...|          Kevin Lima|   110 min|Comedy, Family, F...|    PG|        2007|    s20|           Enchanted|  Movie|\n",
      "+--------------------+----+-----------------+--------------------+--------------------+----------+--------------------+------+------------+-------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json('/Users/shubhangigupta/Desktop/log_filedndjson_file/data.json', sampleSchema, multiLine=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ac742e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788f96c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
